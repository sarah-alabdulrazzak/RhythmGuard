{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is used for reading the original signals (ECG, PPG, ABP) from the CSV files, and extract features and labels from them. Then saving a dataset of those features & labels to be used for the train & test processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # for the progress bar :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Process_Instant(Signals, Subset_Size = 125*3): # defulat Subset_Size to cover 3 seconds of data samples\n",
    "    \n",
    "    import numpy as np\n",
    "    from scipy.signal import find_peaks\n",
    "    import pandas as pd\n",
    "    \n",
    "    N_tot = len(Signals.index) # Total number of samples\n",
    "    N_ps = Subset_Size # Num of points per subset\n",
    "    N_ss = N_tot//N_ps # Number of subsets (each with N_ps points), discarding the last points < N_ps\n",
    "    All_data=pd.DataFrame() # an empty DF to store the parameters we got from the processed data \n",
    "    fs=125 # used to convert PPTp from Number_of_samples to time (same as in main. If changed, pass it to this fn)\n",
    "    \n",
    "    for i in range(N_ss):  # USE find_peaks_cwt instead??\n",
    "           \n",
    "        # I found, imperically, that find_peaks works better for PPG compared with ECG & ABP\n",
    "        # So, I am detecting PPG peaks fisrt, then using 0.8*its average distance to be the min spacing for ECG & ABP peaks!\n",
    "        # Hopefully this may return more accurate PTT calculations\n",
    "        \n",
    "        \n",
    "        # PPG peaks' locations\n",
    "        PPG_peaks, _ = find_peaks(Signals['PPG'][i*N_ps:(i+1)*N_ps], distance=40)\n",
    "        if len(PPG_peaks)<2: \n",
    "            print('PPG')\n",
    "            continue\n",
    "        \n",
    "        New_dist = 0.8*np.mean(np.diff(PPG_peaks))\n",
    "        \n",
    "        # PPG min' locations\n",
    "        PPG_min, _ = find_peaks(-1*Signals['PPG'][i*N_ps:(i+1)*N_ps], distance=New_dist)\n",
    "        if len(PPG_min)<2: \n",
    "            print('PPG')\n",
    "            continue\n",
    "        \n",
    "        # ECG peaks' locations & MAP \n",
    "        ECG_peaks, _ = find_peaks(Signals['ECG'][i*N_ps:(i+1)*N_ps], distance=New_dist) \n",
    "        MAP = np.mean(Signals['ABP'].iloc[i*N_ps + np.arange(ECG_peaks[0], ECG_peaks[-1]+1)].values) #### VALUES not indicies\n",
    "        if len(ECG_peaks)<2: \n",
    "            print('ECG')\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if PPG_peaks[-1]<ECG_peaks[0]: \n",
    "            print('ECG lagging too much')\n",
    "            continue\n",
    "        \n",
    "        # PPT Calculations:\n",
    "        # The 1st PPG peak has to lag the 1st ECG peak, if not then drop it and consider the next one.\n",
    "        Lp, Le, Lm = len(PPG_peaks), len(ECG_peaks), len(PPG_min)\n",
    "        min_pe = min(Lp, Le, Lm)\n",
    "        Lpe = min(Lp,Le)\n",
    "        Lme = min(Lm,Le)\n",
    "        Lmp = min(Lp,Lm)\n",
    "        \n",
    "        if (PPG_peaks[0]>ECG_peaks[0]) and (PPG_min[0]>PPG_peaks[0]): # may add more conditions here for ACCEPTABLE PTT values\n",
    "            PTT_vec = PPG_peaks[:Lpe] - ECG_peaks[:Lpe]\n",
    "            PTT_min_vec = PPG_min[:Lme] - ECG_peaks[:Lme]\n",
    "            PTT_h_vec = Signals['PPG'].iloc[i*N_ps + PPG_peaks[:Lmp]].values - Signals['PPG'].iloc[i*N_ps + PPG_min[:Lmp]].values #### VALUES not indicies\n",
    "            \n",
    "        else: # start from the 2nd PPG peak all the way to one before the last ECG peak\n",
    "            s = 1\n",
    "            while (PPG_peaks[s]<ECG_peaks[0]) and (s<=Lp-1):\n",
    "                s+=1\n",
    "            ind_max_ECG = min(Lp-s,Le) # common last elem index between max & ECG vectors\n",
    "            PTT_vec = PPG_peaks[s:s+ind_max_ECG] - ECG_peaks[:ind_max_ECG] # PATp\n",
    "            \n",
    "            # Now making sure that min's follows max's\n",
    "            r = 0\n",
    "            while (PPG_min[r]<PPG_peaks[s]) and (r<=Lm-1):\n",
    "                r+=1\n",
    "            ind_m_ECG = min(Lm-r,Le) # common last elem index between min & ECG vectors\n",
    "            PTT_min_vec = PPG_min[r:r+ind_m_ECG] - ECG_peaks[:ind_m_ECG] # PATf\n",
    "            ind_h_min = min(Lp-s, Lm-r) # common last elem index between min & max vectors\n",
    "            PTT_h_vec = Signals['PPG'].iloc[i*N_ps + PPG_peaks[s:s+ind_h_min]].values - Signals['PPG'].iloc[i*N_ps + PPG_min[r:r+ind_h_min]].values # PPGmax - PPGmin (i.e. PPG height)\n",
    "        \n",
    "        PTT_vec_p = [x for x in PTT_vec if x>0] # forcing positive values\n",
    "        PTT_min_vec_p = [x for x in PTT_min_vec if x>0]\n",
    "        PTT_h_vec_p = [x for x in PTT_h_vec if x>0]\n",
    "        \n",
    "        PTT_avg = np.mean(PTT_vec_p)\n",
    "        PTT_min_avg = np.mean(PTT_min_vec_p)\n",
    "        PTT_h_avg = np.mean(PTT_h_vec_p)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # BP Min & Max Peaks' (DBP & SBP) locations\n",
    "        BPmax_peaks, _ = find_peaks(Signals['ABP'][i*N_ps:(i+1)*N_ps], distance=New_dist) # indices of SBP\n",
    "        BPmin_peaks, _ = find_peaks(-1*Signals['ABP'][i*N_ps:(i+1)*N_ps], distance=New_dist) # indices of DBP\n",
    "        \n",
    "        \n",
    "            # Calculate HR by averaging distances from ECG, PPG, BPmax, BPmin\n",
    "        HR_ECG = np.mean(np.diff(ECG_peaks))\n",
    "        HR_PPG = np.mean(np.diff(PPG_peaks))\n",
    "        HR_PPG_min = np.mean(np.diff(PPG_min))\n",
    "        HR_SBP = np.mean(np.diff(BPmax_peaks))\n",
    "        HR_DBP = np.mean(np.diff(BPmin_peaks))\n",
    "        HR_avg = 125*60/((HR_ECG + HR_PPG + HR_SBP + HR_DBP + HR_PPG_min)/5) # HR per minute\n",
    "        ############################ 125*60/(mean(diff))\n",
    "        \n",
    "        # Creating DF's of SBP, DBP, and HR along with the time axis to be merged with the Subset DF\n",
    "        SBP_avg = np.mean(Signals.iloc[i*N_ps + BPmax_peaks].loc[:,['ABP']])\n",
    "        DBP_avg = np.mean(Signals.iloc[i*N_ps + BPmin_peaks].loc[:,['ABP']]) \n",
    "        \n",
    "        \n",
    "        # Append log_PPT, log-scale is more convinient for estimating BP (inspired from an approx eqn)\n",
    "        Subset_DF = pd.DataFrame({'PTT':np.log(PTT_avg/fs), \n",
    "                                  'PTTm':np.log(PTT_min_avg/fs), \n",
    "                                  'PTTh':PTT_h_avg,\n",
    "                                  'SBP':SBP_avg,\n",
    "                                  'DBP':DBP_avg,\n",
    "                                  'MAP':MAP,\n",
    "                                  'HR':HR_avg})\n",
    "         \n",
    "        \n",
    "        # Finally, append the DF of the current subset to the whole DF for all patients (may add more numerics, ID, ... later)\n",
    "        All_data=All_data.append(Subset_DF, ignore_index=True)\n",
    "    return All_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and calculating indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'C:/1_Work/FANOS_lab/Reduced_Processed_Dataset_UCI_Kauchee/Generated_CSVs_from_Reduced_Dataset/Single_File/All_Signals.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0065c0f63484>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Now read data header-less then give the col names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mSignals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mData_Dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mFile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#.reset_index()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mSignals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'PPG'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ABP'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ECG'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# adding columns' names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'C:/1_Work/FANOS_lab/Reduced_Processed_Dataset_UCI_Kauchee/Generated_CSVs_from_Reduced_Dataset/Single_File/All_Signals.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# for single csv file of 1,000 instants, use this directory & change file name as needed\n",
    "#Data_Dir = \"C:/1_Work/FANOS_lab/Reduced_Processed_Dataset_UCI_Kauchee/Generated_CSVs_from_Reduced_Dataset/12_Files_1000_instant_each/\"\n",
    "#File_name = \"1.csv\" # files 1 - 12, each with 1,000 instants\n",
    "\n",
    "# To load all data from a single file: \n",
    "Data_Dir = \"C:/1_Work/FANOS_lab/Reduced_Processed_Dataset_UCI_Kauchee/Generated_CSVs_from_Reduced_Dataset/Single_File/\"\n",
    "File_name = \"All_Signals.csv\" # All instants, 12,000, in a single csv file\n",
    "\n",
    "# Now read data header-less then give the col names\n",
    "Signals = pd.read_csv(Data_Dir + File_name, header=None) #.reset_index()\n",
    "Signals.columns = ['PPG','ABP','ECG'] # adding columns' names\n",
    "\n",
    "# Reading the instants' lengthes to be used for calculating indices\n",
    "Data_Indices = pd.read_csv(Data_Dir+\"Instants_Length.csv\", header=None)\n",
    "Data_Indices.columns = ['Num_Samples']\n",
    "\n",
    "# Calculating Instant Indices, Start & End\n",
    "End_indices = np.cumsum(Data_Indices[\"Num_Samples\"])\n",
    "Start_indices = np.append(0, End_indices) \n",
    "\n",
    "# Sampling frequency\n",
    "Fs=125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction: Converting signals into PTT, PTTm, PTTh, HR, SBP, DBP, MAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/12000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Process_Instant' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2828/2245493783.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mID\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInstants_ID\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mInstant_DF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mProcess_Instant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSignals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mStart_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mEnd_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSubset_Size\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Processing data for 1 instant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mAll_Instants_Data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAll_Instants_Data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInstant_DF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Appending to the whole processed dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Process_Instant' is not defined"
     ]
    }
   ],
   "source": [
    "# Looping over the instants, processing them one by one, storing processed data (PTT, SBP, DBP) in another dataframe\n",
    "All_Instants_Data = pd.DataFrame() \n",
    "\n",
    "# Which instants would you like to explore?\n",
    "iStart, iEnd = 1, 12000 # 1:12,000 \n",
    "Instants_ID = np.arange(iStart - 1, iEnd)\n",
    "Subset_Size = 8*125 # how many seconds to take at a time\n",
    "\n",
    "for ID in tqdm(Instants_ID):  \n",
    "    Instant_DF = Process_Instant(Signals.iloc[Start_indices[ID]:End_indices[ID]+1], Subset_Size)  # Processing data for 1 instant\n",
    "    All_Instants_Data = All_Instants_Data.append(Instant_DF, ignore_index=True) # Appending to the whole processed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and save the generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PTTh</th>\n",
       "      <th>PTTm</th>\n",
       "      <th>PTT</th>\n",
       "      <th>HR</th>\n",
       "      <th>SBP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>MAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>224017.000000</td>\n",
       "      <td>224017.000000</td>\n",
       "      <td>224017.000000</td>\n",
       "      <td>224017.000000</td>\n",
       "      <td>224017.000000</td>\n",
       "      <td>224017.000000</td>\n",
       "      <td>224017.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.502436</td>\n",
       "      <td>1.125356</td>\n",
       "      <td>0.575596</td>\n",
       "      <td>92.394599</td>\n",
       "      <td>133.576023</td>\n",
       "      <td>71.387614</td>\n",
       "      <td>93.667467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.471776</td>\n",
       "      <td>0.467996</td>\n",
       "      <td>0.328781</td>\n",
       "      <td>15.733808</td>\n",
       "      <td>20.316135</td>\n",
       "      <td>9.771469</td>\n",
       "      <td>11.612897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.132734</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>54.415423</td>\n",
       "      <td>80.019000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>66.646760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.391544</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>0.432667</td>\n",
       "      <td>80.456582</td>\n",
       "      <td>118.808333</td>\n",
       "      <td>64.047526</td>\n",
       "      <td>85.024067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.673225</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>0.612571</td>\n",
       "      <td>91.519219</td>\n",
       "      <td>132.973333</td>\n",
       "      <td>68.917933</td>\n",
       "      <td>91.924155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.827060</td>\n",
       "      <td>1.282667</td>\n",
       "      <td>0.664615</td>\n",
       "      <td>102.120974</td>\n",
       "      <td>148.020000</td>\n",
       "      <td>76.167333</td>\n",
       "      <td>100.996230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.608299</td>\n",
       "      <td>4.086000</td>\n",
       "      <td>2.256800</td>\n",
       "      <td>155.795596</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>129.955385</td>\n",
       "      <td>148.771474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                PTTh           PTTm            PTT             HR  \\\n",
       "count  224017.000000  224017.000000  224017.000000  224017.000000   \n",
       "mean        1.502436       1.125356       0.575596      92.394599   \n",
       "std         0.471776       0.467996       0.328781      15.733808   \n",
       "min         0.132734       0.030500       0.008000      54.415423   \n",
       "25%         1.391544       0.924800       0.432667      80.456582   \n",
       "50%         1.673225       1.120000       0.612571      91.519219   \n",
       "75%         1.827060       1.282667       0.664615     102.120974   \n",
       "max         2.608299       4.086000       2.256800     155.795596   \n",
       "\n",
       "                 SBP            DBP            MAP  \n",
       "count  224017.000000  224017.000000  224017.000000  \n",
       "mean      133.576023      71.387614      93.667467  \n",
       "std        20.316135       9.771469      11.612897  \n",
       "min        80.019000      60.000000      66.646760  \n",
       "25%       118.808333      64.047526      85.024067  \n",
       "50%       132.973333      68.917933      91.924155  \n",
       "75%       148.020000      76.167333     100.996230  \n",
       "max       180.000000     129.955385     148.771474  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Instants_Data.describe() # check on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Instants_Data.to_csv('Extracted_Instants_Parameters_8secWindow_PTTm_PTTh_MAP_210621.csv') # Saving Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_test4",
   "language": "python",
   "name": "env_test4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
